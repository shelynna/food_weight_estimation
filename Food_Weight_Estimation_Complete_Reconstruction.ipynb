{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b3b046",
   "metadata": {},
   "source": [
    "# Food Weight Estimation - Complete Project Reconstruction\n",
    "\n",
    "This notebook implements the **complete rebuild** of the food weight estimation pipeline using:\n",
    "- **CV Backend**: TensorFlow 1 frozen model with correct logic (`pixel != 254`, `* 0.015`)\n",
    "- **MLLM Frontend**: LLaVA 1.5 7B fine-tuned with LoRA\n",
    "- **Stable Stack**: Carefully pinned library versions to avoid dependency conflicts\n",
    "\n",
    "## ⚠️ Important: Hard Reset Colab Runtime\n",
    "\n",
    "Before running this notebook:\n",
    "1. Go to **Runtime** in the top menu\n",
    "2. Select **Disconnect and Delete Runtime**\n",
    "3. Click **Yes** and wait for it to reconnect\n",
    "4. Run cells one by one from top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dff27",
   "metadata": {},
   "source": [
    "## Section 1: Setup - Mount Drive & Define Paths\n",
    "\n",
    "Mount Google Drive and define all project paths for images, models, CSV files, and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedff599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup: Mount Drive & Define Paths\n",
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define Your Existing Paths\n",
    "ROOT_DIR = \"/content/drive/MyDrive/MyFoodProject\"\n",
    "\n",
    "# --- INPUTS (Your existing data) ---\n",
    "IMAGE_DIR       = os.path.join(ROOT_DIR, \"images_with_gt_weights\")\n",
    "TF_MODEL_DIR    = os.path.join(ROOT_DIR, \"tf_portion_model\")\n",
    "MASTER_CSV_PATH = os.path.join(ROOT_DIR, \"ghana_gt_weights_w_filenames_images.csv\")\n",
    "TF_MODEL_FILE   = os.path.join(TF_MODEL_DIR, \"ghana_frozen_graph_9.0_489ksteps.pb\")\n",
    "\n",
    "# --- OUTPUTS (Files we will create right now) ---\n",
    "DATA_DIR          = os.path.join(ROOT_DIR, \"data\")\n",
    "CHECKPOINT_DIR    = os.path.join(ROOT_DIR, \"food_llm_v1\")\n",
    "FINAL_ADAPTER_DIR = os.path.join(ROOT_DIR, \"final_adapter\")\n",
    "TRAIN_SUBSET_CSV  = os.path.join(ROOT_DIR, \"ghana_train_subset.csv\")\n",
    "DATASET_JSON_PATH = os.path.join(DATA_DIR, \"dataset.json\")\n",
    "TF_SCRIPT_PATH    = os.path.join(ROOT_DIR, \"run_tf_inference.py\")\n",
    "\n",
    "# 3. Create Output Folders\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_ADAPTER_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {ROOT_DIR}\")\n",
    "print(f\"Model File: {TF_MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c21749",
   "metadata": {},
   "source": [
    "## Section 2: Install Dependencies (Stable Stack)\n",
    "\n",
    "Install the stable combination of PyTorch, Transformers, PEFT, and TensorFlow that avoids dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Install Dependencies (Stable Stack)\n",
    "print(\"Installing LLaVA dependencies (PyTorch)...\")\n",
    "!pip install -q \"transformers==4.39.3\"\n",
    "!pip install -q \"peft==0.9.0\"\n",
    "!pip install -q \"accelerate==0.29.3\"\n",
    "!pip install -q \"bitsandbytes==0.43.0\"\n",
    "!pip install -q \"datasets==2.16.1\"\n",
    "!pip install -q \"sentencepiece\"\n",
    "\n",
    "print(\"Installing CV dependencies (TensorFlow)...\")\n",
    "# Fix for the 'ml_dtypes' error\n",
    "!pip install -q \"ml_dtypes>=0.5.0\"\n",
    "!pip install -q \"tensorflow==2.17.0\"\n",
    "\n",
    "print(\"Installing Utilities...\")\n",
    "!pip install -q \"pandas==2.0.3\" \"tqdm==4.66.1\" \"Pillow==10.0.1\" \"opencv-python-headless\"\n",
    "\n",
    "print(\"Installation complete. Waiting 10 seconds before next cell.\")\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f9356",
   "metadata": {},
   "source": [
    "## Section 3: Create CV Inference Script\n",
    "\n",
    "Write the `run_tf_inference.py` script with correct logic:\n",
    "- Load frozen TensorFlow graph\n",
    "- Preprocess images with OpenCV\n",
    "- Run segmentation inference\n",
    "- Count non-background pixels (pixel != 254)\n",
    "- Convert to grams using 0.015 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a21890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Create CV Inference Script (Recreating run_tf_inference.py)\n",
    "%%writefile $TF_SCRIPT_PATH\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Force TF1 compatibility\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "def load_graph(frozen_graph_filename):\n",
    "    \"\"\"Loads the .pb model.\"\"\"\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "    return graph\n",
    "\n",
    "def preprocess_image(image_path, width=513, height=513):\n",
    "    \"\"\"Loads image using OpenCV and resizes.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None: return None\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(img_rgb, (width, height))\n",
    "        return np.expand_dims(img_resized, axis=0)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_gram_weight(sess, image_tensor, output_tensor, image_path):\n",
    "    \"\"\"\n",
    "    LOGIC:\n",
    "    1. Run CV model.\n",
    "    2. Count pixels that are NOT 254 (Background).\n",
    "    3. Multiply by 0.015 to get grams.\n",
    "    \"\"\"\n",
    "    image_np = preprocess_image(image_path)\n",
    "    if image_np is None: return None\n",
    "\n",
    "    seg_map = sess.run(output_tensor, feed_dict={image_tensor: image_np})\n",
    "    seg_map = np.squeeze(seg_map)\n",
    "\n",
    "    # --- THE CORE LOGIC ---\n",
    "    # Count pixels that are NOT the background class (254)\n",
    "    pixel_count = np.sum(seg_map != 254)\n",
    "    \n",
    "    # Apply your specific conversion factor\n",
    "    gram_weight = float(pixel_count * 0.015)\n",
    "    # ----------------------\n",
    "    \n",
    "    return gram_weight\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\", required=True)\n",
    "    parser.add_argument(\"--csv_path\", required=True)\n",
    "    parser.add_argument(\"--image_dir\", required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load Model\n",
    "    print(\"Loading Graph...\", file=sys.stderr)\n",
    "    graph = load_graph(args.model_path)\n",
    "    image_tensor = graph.get_tensor_by_name(\"ImageTensor:0\")\n",
    "    output_tensor = graph.get_tensor_by_name(\"SemanticPredictions:0\")\n",
    "\n",
    "    # Load Data\n",
    "    df = pd.read_csv(args.csv_path)\n",
    "    results = {}\n",
    "\n",
    "    # Run Batch\n",
    "    print(\"Starting Session...\", file=sys.stderr)\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), file=sys.stderr):\n",
    "            # Ensure we use 'Filename' column\n",
    "            filename = str(row['Filename'])\n",
    "            path = os.path.join(args.image_dir, filename)\n",
    "            \n",
    "            if os.path.exists(path):\n",
    "                weight = get_gram_weight(sess, image_tensor, output_tensor, path)\n",
    "                results[filename] = weight\n",
    "            else:\n",
    "                results[filename] = None\n",
    "\n",
    "    # Output JSON to stdout\n",
    "    print(json.dumps(results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21f76a",
   "metadata": {},
   "source": [
    "## Section 4: Generate Training Data\n",
    "\n",
    "Load the master CSV, create a training subset, run CV model in batch, collect pixel-based weight hints, and format into LLaVA conversation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Generate Training Data (Batch Process)\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. Create a subset of 150 images for training\n",
    "print(\"Reading Master CSV...\")\n",
    "df_master = pd.read_csv(MASTER_CSV_PATH)\n",
    "# Ensure we have the correct columns: 'Filename', 'weight', and 'GT Food name'\n",
    "required_cols = ['Filename', 'weight', 'GT Food name']\n",
    "if not all(col in df_master.columns for col in required_cols):\n",
    "    print(f\"ERROR: CSV missing columns. Found: {df_master.columns}\")\n",
    "else:\n",
    "    df_subset = df_master.sample(n=min(150, len(df_master)))\n",
    "    df_subset.to_csv(TRAIN_SUBSET_CSV, index=False)\n",
    "\n",
    "    # 2. Run the CV Script via Subprocess\n",
    "    print(\"Running CV Model (TensorFlow 1)... this takes about 2 minutes...\")\n",
    "    command = [\n",
    "        \"python3\", TF_SCRIPT_PATH,\n",
    "        \"--model_path\", TF_MODEL_FILE,\n",
    "        \"--csv_path\", TRAIN_SUBSET_CSV,\n",
    "        \"--image_dir\", IMAGE_DIR\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')\n",
    "        cv_results = json.loads(result.stdout)\n",
    "        print(f\"CV processing complete. Processed {len(cv_results)} images.\")\n",
    "    except Exception as e:\n",
    "        print(f\"CV error: {e}\")\n",
    "        if 'result' in locals() and result.stderr:\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "        cv_results = {}\n",
    "\n",
    "    # 3. Create the LLaVA Dataset\n",
    "    print(\"Creating LLaVA dataset JSON...\")\n",
    "    dataset = []\n",
    "    count = 0\n",
    "\n",
    "    for _, row in df_subset.iterrows():\n",
    "        filename = str(row['Filename'])\n",
    "        gt_name = str(row['GT Food name'])\n",
    "        gt_weight = float(row['weight'])\n",
    "        \n",
    "        # Get the CV hint we just calculated\n",
    "        cv_hint = cv_results.get(filename)\n",
    "        \n",
    "        if cv_hint is not None:\n",
    "            human_prompt = (\n",
    "                f\"<image>\\n\"\n",
    "                f\"Based on a preliminary CV analysis suggesting a total weight of \"\n",
    "                f\"around {cv_hint:.0f}g, provide a detailed breakdown of the \"\n",
    "                f\"food items and their estimated weights in grams.\"\n",
    "            )\n",
    "            gpt_answer = f\"Here is the breakdown:\\n- {gt_name}: {gt_weight:.1f}g\"\n",
    "            \n",
    "            dataset.append({\n",
    "                \"id\": filename,\n",
    "                \"image\": filename,\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": human_prompt},\n",
    "                    {\"from\": \"gpt\", \"value\": gpt_answer}\n",
    "                ]\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "    with open(DATASET_JSON_PATH, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "\n",
    "    print(f\"Dataset created with {count} samples at: {DATASET_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee94d35",
   "metadata": {},
   "source": [
    "## Section 5: Train LLaVA Model\n",
    "\n",
    "Load LLaVA 1.5 7B with 4-bit quantization, apply LoRA adapters, create data collator, and fine-tune on generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28886ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Train LLaVA (Fine-Tuning)\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. Config\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# 2. Load Model\n",
    "print(\"Loading LLaVA...\")\n",
    "model = LlavaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 3. Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 4. Data Collator\n",
    "class LlavaCollator:\n",
    "    def __init__(self, processor): self.processor = processor\n",
    "    def __call__(self, features):\n",
    "        images = []\n",
    "        prompts = []\n",
    "        for f in features:\n",
    "            try:\n",
    "                img_path = os.path.join(IMAGE_DIR, f['image'])\n",
    "                images.append(Image.open(img_path).convert('RGB'))\n",
    "                prompts.append(processor.tokenizer.apply_chat_template(f['conversations'], tokenize=False))\n",
    "            except: continue\n",
    "        \n",
    "        batch = self.processor(text=prompts, images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        batch['labels'] = batch['input_ids'].clone()\n",
    "        return batch\n",
    "\n",
    "# 5. Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=load_dataset(\"json\", data_files=DATASET_JSON_PATH, split=\"train\"),\n",
    "    data_collator=LlavaCollator(processor)\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving adapter...\")\n",
    "model.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "processor.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "print(f\"Training complete. Adapter saved to {FINAL_ADAPTER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecaa07",
   "metadata": {},
   "source": [
    "## Section 6: Verify and Test Model\n",
    "\n",
    "Load fine-tuned model with trained adapter, select test image, run CV helper to get weight hint, generate LLaVA output, and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e16ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Test the Model\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "\n",
    "# 1. Load Trained Model\n",
    "print(\"Loading trained model...\")\n",
    "base_model = LlavaForCausalLM.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, FINAL_ADAPTER_DIR).merge_and_unload()\n",
    "processor = AutoProcessor.from_pretrained(FINAL_ADAPTER_DIR)\n",
    "\n",
    "# 2. Pick a Test Image\n",
    "df_test = pd.read_csv(TRAIN_SUBSET_CSV).sample(1).iloc[0]\n",
    "img_path = os.path.join(IMAGE_DIR, df_test['Filename'])\n",
    "\n",
    "# 3. Get CV Hint (On the fly)\n",
    "def get_single_cv_hint(path):\n",
    "    dummy_csv = \"dummy.csv\"\n",
    "    pd.DataFrame([{\"Filename\": os.path.basename(path)}]).to_csv(dummy_csv, index=False)\n",
    "    cmd = [\"python3\", TF_SCRIPT_PATH, \"--model_path\", TF_MODEL_FILE, \"--csv_path\", dummy_csv, \"--image_dir\", os.path.dirname(path)]\n",
    "    try:\n",
    "        res = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')\n",
    "        return json.loads(res.stdout).get(os.path.basename(path), 0)\n",
    "    except: return 0\n",
    "\n",
    "cv_hint = get_single_cv_hint(img_path)\n",
    "\n",
    "# 4. Generate LLaVA Response\n",
    "prompt = f\"USER: <image>\\nBased on a preliminary CV analysis suggesting a total weight of around {cv_hint:.0f}g, provide a detailed breakdown of the food items and their estimated weights in grams.\\nASSISTANT:\"\n",
    "inputs = processor(prompt, images=Image.open(img_path), return_tensors=\"pt\").to(\"cuda\")\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "response = processor.decode(out[0], skip_special_tokens=True).split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "# 5. Show Result\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Food: {df_test['GT Food name']}\")\n",
    "print(f\"Ground Truth: {df_test['weight']}g\")\n",
    "print(f\"CV Hint: {cv_hint:.0f}g\")\n",
    "print(f\"Model Prediction: {response}\")\n",
    "print(\"=\"*40)\n",
    "display_img = cv2.resize(cv2.imread(img_path), (300, 300))\n",
    "cv2_imshow(display_img)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
